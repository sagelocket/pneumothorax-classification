{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2542a436","outputId":"51f68f44-80f3-4a34-fecc-9fb0ced09dcc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pydicom in /usr/local/lib/python3.7/dist-packages (2.3.0)\n"]}],"source":["!pip install pydicom\n","import numpy as np \n","import pandas as pd\n","import pydicom,os,cv2\n","from glob import glob\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","plt.style.use('seaborn-whitegrid')\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","%matplotlib inline\n","\n","warnings.filterwarnings(\"ignore\")"],"id":"2542a436"},{"cell_type":"markdown","metadata":{"id":"UyaJiHX80Vgs"},"source":["Downloading various python libraries to help clean, present, and augment data to be fed to the model. \n","\n","- Numpy was used to help process and clean the data\n","- Pandas was used to help create a dataframe that allowed for easy access to the metadata of the images from the training set \n","- Pydicom was used to help obtain the metadata from each image, such as the patients age, sex, view position and so on. \n","- Matplotlib was used to help present the data in graph formats: pie chart, bar graph, and line charts\n","- Torch was used to help create an AI model that can determine whether a set of lungs has pneumothorax or not based on x-rays "],"id":"UyaJiHX80Vgs"},{"cell_type":"code","execution_count":null,"metadata":{"id":"r2CSRZ3qsgMy"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n","!ls\n","data_folder = 'train_png'\n","train_rle_path = '/content/drive/My Drive/Colab Notebooks/pneumothorax/train-rle.csv'\n","\n","print(os.path.exists('train_png'))\n"],"id":"r2CSRZ3qsgMy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccffad48"},"outputs":[],"source":["from glob import glob\n","train = sorted(glob('pneumothorax/dicom-images-train/*/*/*.dcm'))\n","test = sorted(glob('pneumothorax/dicom-images-test/*/*/*.dcm'))\n","\n","print(f'Number of train dicom files in folder:{len(train)}')\n","print(f'Number of test dicom files in folder:{len(test)}')\n","train_rle_path = 'pneumothorax/train-rle.csv'\n","df = pd.read_csv('pneumothorax/train-rle.csv')\n","res = [*set(df)]\n","print(df.shape)\n","\n","print(f'Total no of unique images in csv file: {df[\"ImageId\"].nunique()}')\n","print(f'Images with duplicate Encoded pixels {df[df.duplicated(subset=[\"ImageId\"])].shape[0]}')\n","\n","print(\"-1 means no Pneumothorax\")\n","df.head()"],"id":"ccffad48"},{"cell_type":"code","execution_count":null,"metadata":{"id":"56enS_fWiLzs"},"outputs":[],"source":["\n","#def convert_images(filename, outdir):\n"," #   ds = pydicom.read_file(str(filename))\n","  #  img = ds.pixel_array\n","   # img = cv2.resize(img, (128, 128))\n","    #cv2.imwrite(outdir + filename.split('/')[-1][:-4] + '.png', img)\n","#train_path = 'pneumothorax/dicom-images-train/'\n","#test_path = 'pneumothorax/dicom-images-train/'\n","#train_out_path = 'train_png/'\n","#test_out_path = 'test_png/'\n","#if not os.path.exists(test_out_path):\n"," #   os.makedirs(test_out_path)\n","#import os\n","#import cv2\n","#import glob2\n","#import pydicom\n","#from joblib import Parallel, delayed\n","#from tqdm import tqdm_notebook as tqdm\n","#train_dcm_list = glob2.glob(os.path.join(train_path, '**/*.dcm'))\n","#test_dcm_list = glob2.glob(os.path.join(test_path, '**/*.dcm'))\n","#res1 = Parallel(n_jobs=8, backend='threading')(delayed(\n"," #   convert_images)(i, test_out_path) for i in tqdm(test_dcm_list, total=len(test_dcm_list)))"],"id":"56enS_fWiLzs"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"28250c49"},"outputs":[],"source":["import pydicom\n","import matplotlib.pyplot as plt\n","#displaying the image\n","img = pydicom.read_file(train[0]).pixel_array\n","plt.imshow(img, cmap='bone')\n","plt.grid(False)\n","\n","#displaying metadata\n","data = pydicom.dcmread(train[0])\n","print(data)\n"],"id":"28250c49"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8863b1e7"},"outputs":[],"source":["def show_info(dataset):\n","    print(\"Filename......:\", file_path)\n","    print(\"Storage type.....:\", dataset.SOPClassUID)\n","    print()\n","\n","    pat_name = dataset.PatientName\n","    display_name = pat_name.family_name + \", \" + pat_name.given_name\n","    print(\"Patient's name......:\", display_name)\n","    print(\"Patient id..........:\", dataset.PatientID)\n","    print(\"Patient's Age.......:\", dataset.PatientAge)\n","    print(\"Patient's Sex.......:\", dataset.PatientSex)\n","    print(\"Modality............:\", dataset.Modality)\n","    print(\"Body Part Examined..:\", dataset.BodyPartExamined)\n","    print(\"View Position.......:\", dataset.ViewPosition)\n","    \n","    if 'PixelData' in dataset:\n","        rows = int(dataset.Rows)\n","        cols = int(dataset.Columns)\n","        print(\"Image size.......: {rows:d} x {cols:d}, {size:d} bytes\".format(\n","            rows=rows, cols=cols, size=len(dataset.PixelData)))\n","        if 'PixelSpacing' in dataset:\n","            print(\"Pixel spacing....:\", dataset.PixelSpacing)\n","def plot_pixel(dataset, figsize=(10,10)):\n","    plt.figure(figsize=figsize)\n","    plt.imshow(dataset.pixel_array, cmap=plt.cm.bone)\n","    plt.show()"],"id":"8863b1e7"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"b3222a4a"},"outputs":[],"source":["file_path = train[0]\n","data = pydicom.dcmread(file_path)\n","show_info(data)\n","plot_pixel(data)"],"id":"b3222a4a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"22179727"},"outputs":[],"source":["#dataframe to ease the access\n","patients = []\n","missing = 0\n","\n","pd.reset_option('max_colwidth')\n","\n","for t in train:\n","    data = pydicom.dcmread(t)\n","    patient = {}\n","    patient[\"UID\"] = data.SOPInstanceUID\n","    try:\n","        encoded_pixels = df[df[\"ImageId\"] == patient[\"UID\"]].values[0][1]\n","        patient[\"EncodedPixels\"] = encoded_pixels\n","    except:\n","        missing = missing + 1\n","    patient[\"Age\"] = data.PatientAge\n","    patient[\"Sex\"] = data.PatientSex\n","    patient[\"Modality\"] = data.Modality\n","    patient[\"BodyPart\"] = data.BodyPartExamined\n","    patient[\"ViewPosition\"] = data.ViewPosition\n","    patients.append(patient)\n","\n","print(\"missing labels: \", missing)\n","df_patients = pd.DataFrame(patients, columns=[\"UID\", \"EncodedPixels\", \"Age\", \"Sex\", \"Modality\", \"BodyPart\", \"ViewPosition\"])\n","print(\"images with labels: \", df_patients.shape[0])\n","df_patients.head()"],"id":"22179727"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bb4e6e0d"},"outputs":[],"source":["import matplotlib as mpl\n","import numpy as np\n","\n","#gender\n","men = df_patients[df_patients[\"Sex\"] == \"M\"].shape[0]\n","women = df_patients.shape[0] - men\n","print(men, women)\n","\n","\n","#illness\n","healthy = df_patients[df_patients[\"EncodedPixels\"] == \" -1\"].shape[0]\n","ill = df_patients.shape[0] - healthy\n","print(healthy, ill)\n","\n","#gender + illness\n","men_h = df_patients[(df_patients[\"Sex\"] == \"M\") & (df_patients[\"EncodedPixels\"] == \" -1\")].shape[0]\n","men_ill = men - men_h\n","women_h = df_patients[(df_patients[\"Sex\"] == \"F\") & (df_patients[\"EncodedPixels\"] == \" -1\")].shape[0]\n","women_ill = women - women_h\n","print(men_h, men_ill, women_h, women_ill)\n","\n","perc = [str(round(men_ill/107.12, 1)) + \"% \\n ill\", \"healthy \\n\" + str(round(men_h/107.12, 1)) + \"%\", \"healthy \\n\" + str(round(women_h/107.12, 1)) + \"%\",str(round(women_ill/107.12, 1)) + \"% \\n ill\"]\n","\n","fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n","\n","fig.suptitle(\"4.1 Gender and Pneumothorax distributions\", fontsize=24, y=1.1)\n","\n","mpl.rcParams['font.size'] = 12.0\n","\n","#circle for donut chart\n","circle0 = plt.Circle( (0,0), 0.6, color = 'white')\n","circle1 = plt.Circle( (0,0), 0.4, color = 'white')\n","circle2 = plt.Circle( (0,0), 0.6, color = 'white')\n","\n","#men women\n","ax[0].pie([men, women], labels=[\"men\", \"women\"], colors=[\"#42A5F5\", \"#E57373\"], autopct='%1.1f%%', pctdistance=0.8, startangle=90)\n","ax[0].add_patch(circle0)\n","ax[0].axis('equal')\n","\n","#gender healthy\n","mypie, _ = ax[2].pie([men, women], radius=1.3, labels=[\"men\", \"women\"], colors=[\"#42A5F5\", \"#E57373\"], startangle=90)\n","plt.setp( mypie, width=0.3, edgecolor='white')\n","\n","mypie2, _ = ax[2].pie([ men_ill, men_h, women_h, women_ill], radius = 1.3 - 0.3, labels=perc, labeldistance=0.61,\n","                      colors = [\"#FFB74D\", \"#9CCC65\", \"#9CCC65\", \"#FFB74D\"], startangle=90)\n","plt.setp( mypie2, width=0.4, edgecolor='white')\n","plt.margins(0,0)\n","\n","#healthy ill\n","ax[1].pie([healthy, ill], labels=[\"healthy\", \"ill\"], colors=[\"#9CCC65\", \"#FFB74D\"], autopct='%1.1f%%', pctdistance=0.8, startangle=135)\n","ax[1].add_patch(circle2)\n","ax[1].axis('equal')  \n","\n","plt.tight_layout()\n","plt.show()"],"id":"bb4e6e0d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"48183562"},"outputs":[],"source":["import numpy as np\n","#group into bins the same aged men and women with histogram --> all of them and ill of them\n","\n","#convert he Age column to int\n","df_patients[\"Age\"] = pd.to_numeric(df_patients[\"Age\"])\n","\n","sorted_ages = np.sort(df_patients[\"Age\"].values)\n","print(sorted_ages)"],"id":"48183562"},{"cell_type":"code","execution_count":null,"metadata":{"id":"252d0c54"},"outputs":[],"source":["#calculating all and ill men and women histograms\n","bins = [i for i in range(100)]\n","plt.style.use('seaborn-whitegrid')\n","\n","all_men = np.histogram(df_patients[df_patients[\"Sex\"] == \"M\"][\"Age\"].values, bins=bins)[0]\n","all_women = np.histogram(df_patients[df_patients[\"Sex\"] == \"F\"][\"Age\"].values, bins=bins)[0]\n","\n","ill_men = np.histogram(df_patients[(df_patients[\"Sex\"] == \"M\") & (df_patients[\"EncodedPixels\"] != ' -1')][\"Age\"].values, bins=bins)[0]\n","ill_women = np.histogram(df_patients[(df_patients[\"Sex\"] == \"F\") & (df_patients[\"EncodedPixels\"] != ' -1')][\"Age\"].values, bins=bins)[0]\n","\n","fig, axes = plt.subplots(ncols=2, sharey=True, figsize=(17, 16))\n","\n","fig.suptitle(\"4.3 The presence of Pneumothorax at particular ages and genders\", fontsize=22, y=0.96)\n","\n","axes[0].margins(x=0.1, y=0.01)\n","m1 = axes[0].barh(bins[:-1], all_men, color='#90CAF9')\n","m2 = axes[0].barh(bins[:-1], ill_men, color='#0D47A1')\n","axes[0].set_title('Men', fontsize=18, pad=15)\n","axes[0].invert_xaxis()\n","axes[0].set(yticks=[i*5 for i in range(20)])\n","axes[0].tick_params(axis=\"y\", labelsize=14)\n","axes[0].yaxis.tick_right()\n","axes[0].xaxis.tick_top()\n","axes[0].legend((m1[0], m2[0]), ('healthy', 'with Pneumothorax'), loc=2, prop={'size': 16})\n","\n","locs = axes[0].get_xticks()\n","\n","axes[1].margins(y=0.01)\n","w1 = axes[1].barh(bins[:-1], all_women, color='#EF9A9A')\n","w2 = axes[1].barh(bins[:-1], ill_women, color='#B71C1C')\n","axes[1].set_title('Women', fontsize=18, pad=15)\n","axes[1].xaxis.tick_top()\n","axes[1].set_xticks(locs)\n","axes[1].legend((w1[0], w2[0]), ('healthy', 'with Pneumothorax'), prop={'size': 17})\n","\n","plt.show()"],"id":"252d0c54"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ce5d47e"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","Age =[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99]\n","\n","Pneumo_men = [0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 4, 0, 0, 3, 4, 28, 55, 17, 15, 13, 21, 33, 15, 18, 30, 10, 20, 3, 27, 17, 39, 17, 16, 22, 24, 28, 2, 10, 8, 10, 23, 33, 12, 5, 13, 15, 28, 40, 15, 27, 14, 41, 29, 31, 30, 19, 24, 27, 43, 34, 41, 14, 24, 32, 52, 20, 28,  7,  7,  6, 22, 13, 11, 10,  1,  2,  2,  7, 12,  0,  8,  7,  0,  6,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0, 0, 0, 0, 0]\n","\n","plt.plot(Age, Pneumo_men, color='blue', marker='o')\n","plt.title('4.4 Presence of Pneumothorax in Men at Increasing Ages',fontsize=25)\n","plt.xlabel('Age', fontsize=20)\n","plt.ylabel('Number of Men with Pneumothorax', fontsize=20)\n","plt.grid(True)\n","plt.rcParams['figure.figsize'] = [17,16]\n","plt.show()\n"],"id":"1ce5d47e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"625aca25"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","Women_Age =[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99]\n","\n","Pneumo_Women = [0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  1,  8,  1, 16,  3,  5,  5,  5,  5,  7, 14,  6,  7, 15, 39,  6,  8,  0,  9,  4, 23, 12, 14,  9, 10, 27, 23,  1,  8, 28, 11, 12, 16, 30, 12, 27, 20, 25, 43, 25, 19, 41, 20, 21, 10, 18, 32, 29, 22, 10, 33, 14, 17, 28, 20, 32, 22, 11, 21, 11, 6, 7, 7, 29, 23,  2,  5,  2,  5,  0,  0, 2,  0,  4,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","  0,  0,  0]\n","\n","plt.plot(Age, Pneumo_Women, color='red', marker='o')\n","plt.title('4.5 Presence of Pneumothorax in Women at Increasing Ages',fontsize=25)\n","plt.xlabel('Age', fontsize=20)\n","plt.ylabel('Number of Women with Pneumothorax', fontsize=20)\n","plt.grid(True)\n","plt.rcParams['figure.figsize'] = [17,16]\n","plt.show()"],"id":"625aca25"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7059357"},"outputs":[],"source":["def run_length_decode(rle, height=1024, width=1024, fill_value=1):\n","    component = np.zeros((height, width), np.float32)\n","    component = component.reshape(-1)\n","    rle = np.array([int(s) for s in rle.strip().split(' ')])\n","    rle = rle.reshape(-1, 2)\n","    start = 0\n","    for index, length in rle:\n","        start = start+index\n","        end = start+length\n","        component[start: end] = fill_value\n","        start = end\n","    component = component.reshape(width, height).T\n","    return component\n","\n","def run_length_encode(component):\n","    component = component.T.flatten()\n","    start = np.where(component[1:] > component[:-1])[0]+1\n","    end = np.where(component[:-1] > component[1:])[0]+1\n","    length = end-start\n","    rle = []\n","    for i in range(len(length)):\n","        if i == 0:\n","            rle.extend([start[0], length[0]])\n","        else:\n","            rle.extend([start[i]-end[i-1], length[i]])\n","    rle = ' '.join([str(r) for r in rle])\n","    return rle"],"id":"e7059357"},{"cell_type":"code","execution_count":2,"metadata":{"id":"d1851261","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660062990294,"user_tz":240,"elapsed":19681,"user":{"displayName":"Kunal Bham","userId":"07642996592051805350"}},"outputId":"4ccbe21c-3bea-4210-8905-0c7910eb3622"},"outputs":[{"output_type":"stream","name":"stdout","text":["Notebook started at:  09:36 AM\n","Tensorflow version:  2.8.2\n"]}],"source":["#Start of Model\n","import os, sys, math, re, gc, random\n","from time import time, strftime, gmtime\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from IPython.display import display, HTML\n","\n","import tensorflow as tf\n","import tensorflow.keras.layers as L\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import KFold\n","\n","if not os.path.isdir('tpu_segmentation'):\n","    !git clone -q https://github.com/reyvaz/tpu_segmentation.git\n","    !pip install -qr tpu_segmentation/requirements.txt >/dev/null\n","    !wget -q https://raw.githubusercontent.com/reyvaz/pneumothorax_detection/master/pneumothorax_utils.py\n","from tpu_segmentation import *\n","from pneumothorax_utils import *\n","\n","start_notebook = time()\n","print('Notebook started at: ', current_time_str())\n","print('Tensorflow version: ', tf.__version__)\n","tf.get_logger().setLevel('ERROR')"],"id":"d1851261"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c718fe61"},"outputs":[],"source":["try: tpu\n","except:\n","    try:\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","        print('Running on TPU ', tpu.master())\n","    except ValueError:\n","        tpu = None\n","        print('TPU not found')\n","    if tpu:\n","        tf.config.experimental_connect_to_cluster(tpu)\n","        tf.tpu.experimental.initialize_tpu_system(tpu)\n","        strategy = tf.distribute.TPUStrategy(tpu)\n","    else:\n","        # Default distribution strategy. Works on CPU and single GPU.\n","        strategy = tf.distribute.get_strategy()"],"id":"c718fe61"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dab1e843"},"outputs":[],"source":["GCS_PATH = 'gs://kds-40bc87f88cf0cbbde64e128ce935b6696430f5ec557d0d961d95815c'\n","IMAGE_SIZE = [1024, 1024] # original size of the x-ray images\n","N_CLASSES = 1\n","N_CHANNELS = 1\n","N_REPLICAS = strategy.num_replicas_in_sync\n","classes = ['No Disease', 'Disease']\n","\n","TFRecs_gcs_path = GCS_PATH + '/tfrecs/'\n","TFRECS_TRAIN_RLE = tf.io.gfile.glob(TFRecs_gcs_path + '*train-disease*.tfrec')\n","TFRECS_TRAIN_NORLE = tf.io.gfile.glob(TFRecs_gcs_path + '*train-no-disease*.tfrec')\n","\n","N_TFRECS_MASK = len(TFRECS_TRAIN_RLE)\n","N_TFRECS_NOMASK = len(TFRECS_TRAIN_NORLE)"],"id":"dab1e843"},{"cell_type":"code","source":["N_FOLDS = 5\n","skf = KFold(n_splits=N_FOLDS)\n","folds ={}\n","for fold,(idxT,idxV) in enumerate(skf.split(np.arange(N_TFRECS_MASK))):\n","    folds.update({fold+1: {'val': idxV, 'train': idxT}})\n","del fold\n","\n","def get_fold_file_lists(fold_num, folds=folds, use_unmasked = False):\n","    fold = folds[fold_num]\n","    TRAINING_FILENAMES = [TFRECS_TRAIN_RLE[i] for i in fold['train']]\n","    VALIDATION_FILENAMES = [TFRECS_TRAIN_RLE[i] for i in fold['val']]\n","\n","    if use_unmasked:\n","        TRAINING_FILENAMES += [TFRECS_TRAIN_NORLE[i] for i in fold['train']]\n","        VALIDATION_FILENAMES += [TFRECS_TRAIN_NORLE[i] for i in fold['val']]\n","    return TRAINING_FILENAMES, VALIDATION_FILENAMES\n","\n","masked_examples = count_data_items(TFRECS_TRAIN_RLE)\n","unmasked_examples = count_data_items(TFRECS_TRAIN_NORLE)\n","class_ratio = unmasked_examples/masked_examples\n","\n","print('Number of MASKED examples for training and validation:   ', masked_examples)\n","print('Number of NON MASKED examples for training and validation:', unmasked_examples)"],"metadata":{"id":"L-YVxPdJqE-t"},"id":"L-YVxPdJqE-t","execution_count":null,"outputs":[]},{"cell_type":"code","source":["AUTO = tf.data.experimental.AUTOTUNE\n","\n","def read_tfrecord(example, vars = ('image', 'rle', 'label')):\n","    features = {\n","        'img_id': tf.io.FixedLenFeature([], tf.string), \n","        'image': tf.io.FixedLenFeature([], tf.string), \n","        'rle': tf.io.FixedLenFeature([], tf.string),\n","        'label': tf.io.FixedLenFeature([], tf.int64),\n","        }\n","    features = {k: features[k] for k in vars}\n","    example = tf.io.parse_single_example(example, features)\n","    return [example[var] for var in features]\n","        \n","def load_dataset(filenames, ordered = False):\n","    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n","    if not ordered: \n","        ignore_order = tf.data.Options()\n","        ignore_order.experimental_deterministic = False \n","        dataset = dataset.with_options(ignore_order)\n","    return dataset.map(read_tfrecord, num_parallel_calls=AUTO)"],"metadata":{"id":"ELmtGT2_qE7r"},"id":"ELmtGT2_qE7r","execution_count":null,"outputs":[]},{"cell_type":"code","source":["undersample_filter = lambda L, P: False if L == 0 and tf.random.uniform([]) < P else True\n","# randomly filters-out examples with label 0. L is the label, P is the rate to exclude\n","\n","def decode_resize_inputs(inputs, target_size, image_size = IMAGE_SIZE,\n","                         n_channels = N_CHANNELS, n_classes = N_CLASSES):\n","    (image_data, rle), args = inputs[:2], inputs[2:]\n","    image = tf.image.decode_jpeg(image_data, channels=n_channels)\n","    image = tf.cast(image, tf.float32) / 255.0  \n","    mask = build_mask_array(rle, image_size)\n","\n","    if target_size != image_size:\n","        image = tf.image.resize(image, target_size)\n","        mask = tf.image.resize(mask, target_size)\n","\n","    image = tf.reshape(image, [*target_size, n_channels]) \n","    mask = tf.reshape(mask, [*target_size, n_classes])\n","    return (image, mask, *args)\n","\n","def data_augment(inputs, target_size, \n","                 n_channels = N_CHANNELS, n_classes = N_CLASSES, \n","                 p1=0.50, p2=0.33, p3=0.33, p4=0.75):\n","    \n","    (image, mask, label), args = inputs[:3], inputs[3:]\n","\n","    if tf.random.uniform([]) < p1:\n","        image, mask = left_right_flip(image, mask)\n","\n","    if tf.random.uniform([]) < p2:\n","        image, mask = random_rotate(image, target_size, n_channels, mask, \n","                                    n_classes, 7.)\n","    elif tf.random.uniform([]) < p2:\n","        image, mask = random_shear(image, target_size, n_channels, mask)\n","\n","    if tf.random.uniform([]) < p3: \n","        image, mask = random_zoom_out_and_pan(image, target_size, mask, n_channels)\n","    elif tf.random.uniform([]) < p3*1.5: \n","        image, mask = image_mask_zoom_in(image, mask, target_size, label, n_channels)\n","    \n","    image = tf.image.random_brightness(image, 0.1)\n","    image = tf.image.random_contrast(image, 0.7, 1.4)\n","\n","    if tf.random.uniform([]) < p4: \n","        image = coarse_dropout(image, target_size, n_channels, \n","                               count_range=(20, 150), m_size = 0.01)\n","    return (image, mask, label, *args)\n","\n","def final_reshape(inputs, target_size, target_var, make_rgb, augment,\n","                  n_channels = N_CHANNELS, n_classes = N_CLASSES):\n","    '''\n","    Converts image to 3 channels if specified by `make_rgb`\n","    Applies augmentations that require 3 channels if requested by `augment`\n","    Returns the image and one of {mask, label}, as specified by `target_var`\n","    '''\n","    (image, mask, label), args = inputs[:3], inputs[3:]\n","\n","    if make_rgb:\n","        image = tf.image.grayscale_to_rgb(image)\n","        n_channels = 3\n","        if augment:\n","            image = tf.image.random_hue(image, 0.025)\n","            image = tf.image.random_saturation(image, 0.6, 1.4)\n","\n","    image = tf.reshape(image, [*target_size, n_channels]) \n","\n","    if target_var == 'mask':\n","        mask = tf.reshape(mask, [*target_size, n_classes])\n","        target_var = mask\n","    elif target_var == 'label': target_var = tf.cast(label, tf.int32)\n","    else: raise Exception('target_var must be one of \\'label\\' or \\'mask\\'')\n","    return image, target_var\n","\n","describe_ds = lambda x: print(re.sub('[<>]', '', str(x)))\n","\n","def print_description(train_ds, val_ds, steps_per_epoch, use_unmasked, \n","                      p_undersample, n_train, n_valid):\n","    describe_ds(train_ds)\n","    print('Steps per epoch: ', steps_per_epoch)\n","    approx = 'approx' if p_undersample and p_undersample !=1 and use_unmasked else ''\n","    print('Num train examples {} {}'.format(n_train, approx))\n","    print('Num valid examples {}'.format(n_valid))\n","    return None"],"metadata":{"id":"-sh88xJIqFBe"},"id":"-sh88xJIqFBe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_dataset(filenames, target_size, batch_size, target_var = 'mask',\n","                make_rgb = True, \n","                augment = False,\n","                cache   = False,\n","                repeat  = False, \n","                shuffle = False,\n","                ordered = False, \n","                drop_remainder = False,\n","                p_undersample = False):\n","    dataset = load_dataset(filenames, ordered)\n","    if p_undersample:\n","        dataset = dataset.filter(lambda *data: undersample_filter(data[2], p_undersample))\n","    dataset = dataset.map(lambda *data: decode_resize_inputs(data, target_size), AUTO)\n","    if augment:\n","        dataset = dataset.map(lambda *data: data_augment(data, target_size), AUTO)\n","    dataset = dataset.map(lambda *data: final_reshape(data, target_size, \n","                                      target_var, make_rgb, augment), AUTO)\n","    if cache: dataset = dataset.cache()  \n","    if repeat: dataset = dataset.repeat() \n","    if shuffle: dataset = dataset.shuffle(shuffle, reshuffle_each_iteration=True) \n","    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n","    dataset = dataset.prefetch(AUTO) \n","    return dataset\n","\n","def get_datasets(fold_num, target_size, imgs_per_replica, target_var = 'mask',\n","             use_unmasked = False, p_undersample = False, cache_val = True, print_descr = True):\n","    TRAINING_FILENAMES, VALIDATION_FILENAMES = get_fold_file_lists(\n","        fold_num, use_unmasked=use_unmasked)\n","    \n","    train_batch_size = imgs_per_replica * N_REPLICAS\n","\n","    n_train = count_data_items(TRAINING_FILENAMES)\n","    n_valid = count_data_items(VALIDATION_FILENAMES)\n","\n","    if p_undersample and use_unmasked:\n","        # approx number of examples given random undersampling, val is never undersampled\n","        n_train = int(n_train * (1-(p_undersample*class_ratio)/(1+class_ratio)))\n","\n","    train_steps = np.ceil(n_train/train_batch_size).astype(int)\n","    buffer_size = int(n_train*0.33)\n","\n","    random.shuffle(TRAINING_FILENAMES)\n","    train_dataset = get_dataset(TRAINING_FILENAMES, target_size, train_batch_size, \n","                                target_var, \n","                                augment = True, \n","                                repeat = True, \n","                                shuffle = buffer_size,\n","                                drop_remainder=True, \n","                                p_undersample = p_undersample)\n","    \n","    val_batch_size = 11 * N_REPLICAS # constant validation dataset for dice metric comparison\n","    # 11 * N_REPLICAS minimizes wasted examples in all folds due to drop_remainder. \n","\n","    val_steps = np.ceil(n_valid/val_batch_size).astype(int)\n","    if target_var == 'mask': val_steps -= 1 # val steps in segmentation due to drop_remainder.\n","\n","    drop_remainder_val = True if target_var == 'mask' else False\n","\n","    valid_dataset = get_dataset(VALIDATION_FILENAMES, target_size, val_batch_size, target_var, \n","                                cache = cache_val, ordered = True,\n","                                drop_remainder = drop_remainder_val)\n","    \n","    if print_descr:\n","        print_description(train_dataset, valid_dataset, train_steps, \n","                          use_unmasked, p_undersample, n_train, n_valid)\n","    steps = {'train': train_steps, 'valid': val_steps}\n","    datasets = {'train': train_dataset, 'valid': valid_dataset}\n","    return datasets, steps"],"metadata":{"id":"b6bDCVrZqFEU"},"id":"b6bDCVrZqFEU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["zip_dest = PROJECT_DIR + 'pneumothorax-fold-1-weights.zip'\n","if ADD_PRETRAINED: \n","    if not os.path.isfile(zip_dest):\n","        !gdown https://drive.google.com/uc?id=1ptjR8KYSg64CZOvp4vVH3GyTROvjETyk -O {zip_dest}\n","    !unzip -qn {zip_dest} -d {weights_dir} "],"metadata":{"id":"DFZZWrIsqFHX"},"id":"DFZZWrIsqFHX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["saved_weights = [w.split('/')[-1] for w in tf.io.gfile.glob(weights_dir+'*.h5')]\n","\n","!mkdir -p {weights_dir}discarded\n","\n","weights_metrics = {}\n","weights_names = {}\n","for w in saved_weights:\n","    prefix, sz, _ , metric = w.split('_')\n","    prefix = '{}_{}'.format(prefix, sz)\n","    metric_float = float(metric.split('.')[0])*10e-6\n","    if not prefix in weights_metrics:\n","        weights_metrics[prefix] = metric_float\n","        weights_names[prefix] = w\n","    elif metric_float > weights_metrics[prefix]:\n","        !mv {weights_dir}{weights_names[prefix]} {weights_dir}discarded/\n","        weights_metrics[prefix] = metric_float\n","        weights_names[prefix] = w\n","    else: \n","        !mv {weights_dir}{w} {weights_dir}discarded/\n","\n","def check_and_save(history, model, fold_num, img_size, metric, metric_abbr = 'acc',\n","                   current_wname = 'weights.h5', weights_dir=weights_dir):\n","    current_metric = max(history.history[metric])\n","    size_str = str(img_size[0]) + 'x' + str(img_size[1])\n","    prefix = '{}-f{}_{}'.format(model.name.lower(), fold_num, size_str)\n","    metric_str = str(current_metric*10e4)[:5]\n","    weights_name = '{}_{}_{}.h5'.format(prefix, metric_abbr, metric_str)\n","    if not prefix in weights_metrics or current_metric >= weights_metrics[prefix]: \n","        weights_names[prefix] = weights_name\n","        if prefix in weights_metrics:\n","            !rm -r {weights_dir}{prefix}*\n","        !cp {current_wname} {weights_dir}{weights_name}\n","        weights_metrics[prefix] = current_metric\n","        saved_weights.append(weights_name)\n","    return None\n","\n","def create_df_row(w):\n","    # w: (str) is the filename of the saved weights\n","    base, size, _, score = w.split('_')\n","    key_id = '{}_{}'.format(base, size)\n","    score = float(score.split('.')[0])*10e-6\n","    size = eval(size.replace('x', ', '))\n","    base, efn_ver, model_type, fold_num = base.split('-')\n","    metric = 'accuracy' if 'bin' in model_type else 'avg image-wise dice'\n","    if 'bin' in model_type: model_type = 'binary' \n","    elif 'pp' in model_type: model_type = 'unet++' \n","    else: model_type = 'unet' \n","    backbone, fold_num = base+efn_ver, int(fold_num[1])\n","    return [key_id, backbone, fold_num, size, score, metric, model_type, w]\n","\n","def get_best_weights(df, n_per_fold = 5, sort_by_col = 'score', \n","                     group_by_col = 'fold', mode = 'max'):\n","    m = df.groupby(group_by_col)[sort_by_col]\n","    m = m.nlargest(n_per_fold) if mode == 'max' else m.nsmallest(n_per_fold)\n","    try: idxs = [i[1] for i in m.index]\n","    except: idxs = [i for i in m.index]\n","    return df.loc[idxs].reset_index(drop=True)"],"metadata":{"id":"qh7DPOqiqFKi"},"id":"qh7DPOqiqFKi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 30\n","use_unmasked = True\n","p_undersample = 0.50 # rate to exclude x-rays labeled 0\n","\n","metric_monitor = performance_monitor('val_accuracy', 'max')\n","loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.25)\n","\n","lr_params =  [3e-4,  3e-4,  1e-6, 2, 4, 8e-1]\n","lr_sched = lr_schedule_builder(lr_params)\n","nadam = tf.keras.optimizers.Nadam() "],"metadata":{"id":"pTdBgi8VqFNI"},"id":"pTdBgi8VqFNI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["folds_to_train = [1] \n","dims_to_train = [512] \n","efficientnet_versions = [1]"],"metadata":{"id":"MTnbVAhBqFP_"},"id":"MTnbVAhBqFP_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["if TRAINING:\n","    for training_fold in folds_to_train:\n","        for dim in dims_to_train: \n","            target_size = (dim, dim)\n","            print(hline)\n","            cache_val, imgs_per_replica = (True, 8) if dim <= 512 else (False, 4)\n","            datasets, steps = get_datasets(training_fold, target_size, imgs_per_replica , \n","                                'label', use_unmasked, p_undersample, cache_val)\n","            INPUT_SHAPE  = (*target_size, 3)\n","            for efn_ver in efficientnet_versions:\n","                base_model = 'EfficientNet-B{}'.format(efn_ver)\n","                print(hline +'\\nTraining {} on FOLD {} with image size {}\\n'.format(\n","                        base_model, training_fold, target_size) + hline)\n","                \n","                with strategy.scope():                       \n","                    model = build_classifier(base_model, N_CLASSES, INPUT_SHAPE, \n","                                                name_suffix='-bin')\n","                    model.compile(optimizer=nadam, loss=loss, metrics=['accuracy', 'AUC']) \n","\n","                checkpoint = config_checkpoint(monitor ='val_accuracy', mode = 'max')\n","                train_begin = time()\n","\n","                history = model.fit(datasets['train'], steps_per_epoch=steps['train'],\n","                        epochs = EPOCHS,\n","                        verbose = 0, \n","                        callbacks=[lr_sched, metric_monitor, checkpoint],\n","                        validation_data = datasets['valid'])\n","                \n","                check_and_save(history, model, training_fold, target_size, 'val_accuracy')\n","                print('Time to train {} epochs: {} (mm:ss)\\n'.format(\n","                    EPOCHS, time_passed(train_begin)))\n","                \n","                del model\n","                K.clear_session()\n","\n","    del training_fold, dim, target_size, datasets, INPUT_SHAPE, imgs_per_replica, history"],"metadata":{"id":"xyZcHfhQqFSm"},"id":"xyZcHfhQqFSm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_fn = glob.glob('/content/drive/MyDrive/Colab Notebooks/test_png/*/*.png')\n","x_test = [cv2.resize(np.array(Image.open(fn)),(img_size,img_size)) for fn in test_fn]\n","x_test = np.array(x_test)\n","x_test = np.array([np.repeat(im[...,None],3,2) for im in x_test])\n","print(x_test.shape)\n","preds_test = model.predict(x_test,batch_size=batch_size)"],"metadata":{"id":"_skF-MWmqFVF"},"id":"_skF-MWmqFVF","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"4iwbfD5hqdlR"},"id":"4iwbfD5hqdlR","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"QnnsKCeIqdoH"},"id":"QnnsKCeIqdoH","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mSJU9pZ-qdq_"},"id":"mSJU9pZ-qdq_","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"9ofhNgg9qdtZ"},"id":"9ofhNgg9qdtZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"PdOAL5MvqdyV"},"id":"PdOAL5MvqdyV","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"d7bhjJixqd1e"},"id":"d7bhjJixqd1e","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2xwu3SqCqd4G"},"id":"2xwu3SqCqd4G","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"h5GF7OBYqd7B"},"id":"h5GF7OBYqd7B","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"fZTI6aWYqd9n"},"id":"fZTI6aWYqd9n","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"23kzQjHjqeAo"},"id":"23kzQjHjqeAo","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"pneumothorax.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}